{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CelcomDigi Snowflake Intelligence - Hands-On Lab\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "This hands-on lab teaches you to analyze telecommunications data using Snowflake Cortex AI functions. You will:\n",
        "\n",
        "1. Query and explore structured and unstructured data\n",
        "2. Use Cortex AISQL functions for text analysis and translation\n",
        "3. Combine customer, network, and call data for insights\n",
        "4. Create visualizations to identify business opportunities\n",
        "5. Leverage Cortex Search for semantic queries\n",
        "6. Calculate business metrics and risk indicators\n",
        "\n",
        "**Prerequisites:** \n",
        "- data_processing.ipynb has been run successfully\n",
        "- **Important:** Add `matplotlib` to notebook packages\n",
        "  - In Snowflake notebook settings\n",
        "  - Click \"Packages\" dropdown\n",
        "  - Type \"matplotlib\" and add it\n",
        "  - This enables all visualizations in this lab\n",
        "\n",
        "**Duration:** 45-60 minutes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "from snowflake.snowpark import Session\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "session = Session.builder.getOrCreate()\n",
        "print(f\"Connected: {session.get_current_database()}.{session.get_current_schema()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Data Exploration and Summary Statistics\n",
        "\n",
        "**What you'll learn:**\n",
        "- Query multiple tables to understand data volume\n",
        "- View sample records from processed transcripts\n",
        "- Verify data loading was successful\n",
        "\n",
        "**Business value:** Understand the scope of available data for analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check all tables\n",
        "tables = ['network_performance', 'customer_details', 'customer_call_transcripts', \n",
        "          'customer_complaint_documents', 'csat_surveys']\n",
        "\n",
        "for table in tables:\n",
        "    count = session.table(table).count()\n",
        "    print(f\"{table:40s}: {count:>6,} records\")\n",
        "\n",
        "# Sample calls\n",
        "print(\"\\\\nSample Calls:\")\n",
        "session.table(\"customer_call_transcripts\").select(\"call_id\", \"call_reason\", \"summary\").show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 2: Multilingual Translation with AI_TRANSLATE\n",
        "\n",
        "**What you'll learn:**\n",
        "- Use AI_TRANSLATE to convert text between languages\n",
        "- Support multilingual teams with automatic translation\n",
        "- Understand supported language codes\n",
        "\n",
        "**Business value:** Enable global teams to analyze customer feedback in their preferred language\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Translate summaries to Chinese and Japanese (for regional teams)\n",
        "# Note: AI_TRANSLATE supports: en, zh, ja, ko, es, fr, de, it, pt, etc.\n",
        "sample = session.sql(\"SELECT summary FROM customer_call_transcripts LIMIT 2\").collect()\n",
        "\n",
        "print(\"Multi-Language Translation Example:\\\\n\")\n",
        "for row in sample:\n",
        "    summary = row['SUMMARY']\n",
        "    print(f\"English: {summary}\")\n",
        "    \n",
        "    # Translate to Chinese\n",
        "    zh = session.sql(f\"SELECT SNOWFLAKE.CORTEX.AI_TRANSLATE('{summary}', 'en', 'zh')\").collect()[0][0]\n",
        "    print(f\"Chinese: {zh}\")\n",
        "    \n",
        "    # Translate to Japanese\n",
        "    ja = session.sql(f\"SELECT SNOWFLAKE.CORTEX.AI_TRANSLATE('{summary}', 'en', 'ja')\").collect()[0][0]\n",
        "    print(f\"Japanese: {ja}\\\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 3: Customer Sentiment Analysis with Visualization\n",
        "\n",
        "**What you'll learn:**\n",
        "- Extract sentiment from AI_SENTIMENT VARIANT results\n",
        "- Aggregate sentiment across all customer calls\n",
        "- Create bar charts to visualize sentiment distribution\n",
        "\n",
        "**Business value:** Identify patterns in customer emotions and satisfaction levels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sentiment distribution\n",
        "sentiment = session.sql(\"\"\"\n",
        "SELECT \n",
        "    sentiment_score:categories[0]:sentiment::VARCHAR as sentiment,\n",
        "    COUNT(*) as count\n",
        "FROM customer_call_transcripts\n",
        "GROUP BY sentiment\n",
        "\"\"\").to_pandas()\n",
        "\n",
        "print(sentiment)\n",
        "\n",
        "# Chart (column names are uppercase from Snowflake)\n",
        "colors = {'positive': 'green', 'negative': 'red', 'neutral': 'gray', 'mixed': 'orange'}\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(sentiment['SENTIMENT'], sentiment['COUNT'], \n",
        "        color=[colors.get(s, 'blue') for s in sentiment['SENTIMENT']])\n",
        "plt.ylabel('Number of Calls')\n",
        "plt.title('Sentiment Distribution')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 4: Customer Satisfaction (CSAT) Analysis\n",
        "\n",
        "**What you'll learn:**\n",
        "- Analyze CSAT score distribution across customer surveys\n",
        "- Calculate average satisfaction metrics\n",
        "- Create histograms to identify satisfaction trends\n",
        "\n",
        "**Business value:** Measure customer satisfaction and track service quality performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CSAT distribution\n",
        "csat = session.sql(\"\"\"\n",
        "SELECT csat_score, COUNT(*) as count\n",
        "FROM csat_surveys\n",
        "GROUP BY csat_score\n",
        "ORDER BY csat_score\n",
        "\"\"\").to_pandas()\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(csat['CSAT_SCORE'], csat['COUNT'], color='teal')\n",
        "plt.xlabel('CSAT Score (1-5)')\n",
        "plt.ylabel('Count')\n",
        "plt.title('CSAT Distribution')\n",
        "plt.xticks([1, 2, 3, 4, 5])\n",
        "plt.show()\n",
        "\n",
        "avg = (csat['CSAT_SCORE'] * csat['COUNT']).sum() / csat['COUNT'].sum()\n",
        "print(f\"Average CSAT: {avg:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 5: Network Performance Analysis by Region\n",
        "\n",
        "**What you'll learn:**\n",
        "- Aggregate network metrics by geographic region\n",
        "- Compare performance across Malaysian states\n",
        "- Identify regions with network quality issues\n",
        "\n",
        "**Business value:** Prioritize infrastructure investments based on regional performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance by region (use all available data)\n",
        "perf = session.sql(\"\"\"\n",
        "SELECT region, \n",
        "       AVG(avg_latency_ms) as latency,\n",
        "       AVG(packet_loss_pct) as packet_loss\n",
        "FROM network_performance\n",
        "GROUP BY region\n",
        "ORDER BY latency DESC\n",
        "\"\"\").to_pandas()\n",
        "\n",
        "print(\"Network Performance by Region:\")\n",
        "print(perf)\n",
        "\n",
        "if len(perf) > 0:\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.barh(perf['REGION'], perf['LATENCY'])\n",
        "    plt.xlabel('Average Latency (ms)')\n",
        "    plt.title('Network Latency by Region')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No network performance data available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 6: Semantic Search with Cortex Search\n",
        "\n",
        "**What you'll learn:**\n",
        "- Perform semantic search across call transcripts and PDF documents\n",
        "- Use natural language queries to find relevant information\n",
        "- Access both structured and unstructured data simultaneously\n",
        "\n",
        "**Business value:** Quickly find relevant customer interactions and documentation without exact keyword matching\n",
        "\n",
        "**Note:** If search service doesn't exist, verify:\n",
        "- `SHOW CORTEX SEARCH SERVICES;` in SQL\n",
        "- Re-run the Cortex Search creation section from setup.sql if needed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search across transcripts and PDFs\n",
        "try:\n",
        "    results = session.sql(\"\"\"\n",
        "    SELECT FEEDBACK_ID, SOURCE, SENTIMENT\n",
        "    FROM TABLE(celcomdigi_feedback_search!SEARCH('network problems', 5))\n",
        "    \"\"\").to_pandas()\n",
        "    print(f\"Found {len(results)} results:\")\n",
        "    print(results[['FEEDBACK_ID', 'SOURCE', 'SENTIMENT']])\n",
        "except Exception as e:\n",
        "    print(f\"Search not ready yet: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 7: Customer Risk Identification and Segmentation\n",
        "\n",
        "**What you'll learn:**\n",
        "- Use customer_360_view to identify at-risk customers\n",
        "- Segment customers by risk level\n",
        "- Visualize risk distribution across customer segments\n",
        "\n",
        "**Business value:** Proactively identify customers likely to churn for retention interventions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find at-risk customers\n",
        "risk = session.sql(\"\"\"\n",
        "SELECT customer_segment, \n",
        "       COUNT(*) as total,\n",
        "       SUM(CASE WHEN is_at_risk THEN 1 ELSE 0 END) as at_risk\n",
        "FROM customer_360_view\n",
        "GROUP BY customer_segment\n",
        "\"\"\").to_pandas()\n",
        "\n",
        "print(risk)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "x = range(len(risk))\n",
        "plt.bar(x, risk['TOTAL'], label='Total', alpha=0.6)\n",
        "plt.bar(x, risk['AT_RISK'], label='At Risk', color='red')\n",
        "plt.xticks(x, risk['CUSTOMER_SEGMENT'])\n",
        "plt.ylabel('Customers')\n",
        "plt.title('Customer Risk by Segment')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 8: Revenue Impact and Risk Quantification\n",
        "\n",
        "**What you'll learn:**\n",
        "- Calculate revenue at risk from dissatisfied customers\n",
        "- Aggregate financial impact by customer segment\n",
        "- Visualize revenue exposure from potential churn\n",
        "\n",
        "**Business value:** Quantify financial impact of customer dissatisfaction to justify retention investments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Revenue at risk\n",
        "revenue = session.sql(\"\"\"\n",
        "SELECT customer_segment,\n",
        "       SUM(CASE WHEN is_at_risk THEN monthly_revenue ELSE 0 END) as revenue_at_risk\n",
        "FROM customer_360_view\n",
        "GROUP BY customer_segment\n",
        "ORDER BY revenue_at_risk DESC\n",
        "\"\"\").to_pandas()\n",
        "\n",
        "print(revenue)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(revenue['CUSTOMER_SEGMENT'], revenue['REVENUE_AT_RISK'], color='darkred')\n",
        "plt.ylabel('Monthly Revenue at Risk (RM)')\n",
        "plt.title('Revenue at Risk by Segment')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\\\nTotal at Risk: RM {revenue['REVENUE_AT_RISK'].sum():,.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 9: Data Privacy with AI_REDACT\n",
        "\n",
        "**What you'll learn:**\n",
        "- Use AI_REDACT to automatically remove PII from text\n",
        "- Protect customer privacy while enabling data analysis\n",
        "- Create anonymized datasets for sharing\n",
        "\n",
        "**Business value:** Comply with data privacy regulations while maintaining analytical capabilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Redact PII from transcript\n",
        "sample = session.sql(\"SELECT transcript_text FROM customer_call_transcripts LIMIT 1\").collect()\n",
        "\n",
        "if len(sample) > 0:\n",
        "    text = sample[0]['TRANSCRIPT_TEXT'][:500]\n",
        "    print(\"Original:\\\\n\", text)\n",
        "    \n",
        "    redacted = session.sql(f\"SELECT SNOWFLAKE.CORTEX.AI_REDACT('{text}')\").collect()[0][0]\n",
        "    print(\"\\\\nRedacted:\\\\n\", redacted)\n",
        "    print(\"\\\\nUse case: Share data while protecting customer privacy\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 10: Custom Analysis and Experimentation\n",
        "\n",
        "**What you'll learn:**\n",
        "- Apply learned concepts to create custom analyses\n",
        "- Combine multiple AI functions for complex insights\n",
        "- Develop your own SQL queries and visualizations\n",
        "\n",
        "**Business value:** Build custom analytics tailored to specific business questions\n",
        "\n",
        "**Try this:**\n",
        "- Analyze competitor mentions and customer sentiment\n",
        "- Create additional visualizations\n",
        "- Experiment with other Cortex AI functions (AI_COMPLETE, AI_EXTRACT, etc.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Calls mentioning competitors\n",
        "competitor = session.sql(\"\"\"\n",
        "SELECT call_id, call_reason,\n",
        "       sentiment_score:categories[0]:sentiment::VARCHAR as sentiment,\n",
        "       summary\n",
        "FROM customer_call_transcripts\n",
        "WHERE LOWER(transcript_text) LIKE '%maxis%' \n",
        "   OR LOWER(transcript_text) LIKE '%mobile%'\n",
        "LIMIT 5\n",
        "\"\"\").to_pandas()\n",
        "\n",
        "print(\"Calls Mentioning Competitors:\")\n",
        "print(competitor)\n",
        "\n",
        "# Your custom analysis here:\n",
        "# - Try different SQL queries\n",
        "# - Use other AI functions (AI_COMPLETE, AI_CLASSIFY)\n",
        "# - Create your own visualizations\n",
        "\n",
        "print(\"\\\\nLab Complete! Now use Snowflake Intelligence Agent for natural language queries.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 11: ML Prediction - Churn Risk Scoring Model\n",
        "\n",
        "**What you'll learn:**\n",
        "- Build a weighted scoring model to predict customer churn\n",
        "- Use multiple features (CSAT, complaints, unresolved issues) for prediction\n",
        "- Compare predicted vs actual churn rates\n",
        "- Visualize model performance with 4 comprehensive charts\n",
        "\n",
        "**Business objectives:**\n",
        "- **Proactive Retention:** Identify at-risk customers before they churn to competitors (Maxis, U Mobile)\n",
        "- **Resource Optimization:** Prioritize retention efforts on high-risk, high-value customers\n",
        "- **Revenue Protection:** Quantify and prevent revenue loss from customer churn\n",
        "- **Operational Efficiency:** Automate churn risk assessment instead of manual analysis\n",
        "\n",
        "**Model features and weights:**\n",
        "- Low CSAT Score (<3): 30 points - Strong predictor of dissatisfaction\n",
        "- High Complaints (>2): 25 points - Indicates ongoing service issues\n",
        "- Unresolved Issues (>0): 25 points - Critical driver of churn\n",
        "- Frequent Calls (>2): 20 points - Sign of persistent problems\n",
        "\n",
        "**Risk threshold:** Score >50 indicates high churn probability\n",
        "\n",
        "**Expected outcome:** Actionable list of customers requiring immediate retention intervention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple churn prediction model\n",
        "churn = session.sql(\"\"\"\n",
        "SELECT \n",
        "    customer_id,\n",
        "    customer_segment,\n",
        "    total_calls,\n",
        "    total_complaints,\n",
        "    avg_csat_score,\n",
        "    unresolved_issues,\n",
        "    is_at_risk,\n",
        "    is_churned\n",
        "FROM customer_360_view c\n",
        "JOIN customer_details d USING (customer_id)\n",
        "WHERE total_calls > 0\n",
        "\"\"\").to_pandas()\n",
        "\n",
        "print(f\"Dataset: {len(churn)} customers\\\\n\")\n",
        "\n",
        "# Calculate churn risk score\n",
        "churn['risk_score'] = (\n",
        "    (churn['AVG_CSAT_SCORE'] < 3).astype(int) * 30 +  # Low satisfaction\n",
        "    (churn['TOTAL_COMPLAINTS'] > 2).astype(int) * 25 +  # Multiple complaints\n",
        "    (churn['UNRESOLVED_ISSUES'] > 0).astype(int) * 25 +  # Unresolved issues\n",
        "    (churn['TOTAL_CALLS'] > 2).astype(int) * 20  # Frequent caller\n",
        ")\n",
        "\n",
        "churn['predicted_churn'] = churn['risk_score'] > 50\n",
        "\n",
        "# Results\n",
        "print(\"Prediction Results:\")\n",
        "print(f\"Predicted High Risk: {churn['predicted_churn'].sum()}\")\n",
        "print(f\"Actual Churned: {churn['IS_CHURNED'].sum()}\")\n",
        "\n",
        "# Visualizations\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# 1. Risk Score Distribution\n",
        "ax1.hist(churn['risk_score'], bins=15, color='coral', edgecolor='black', alpha=0.7)\n",
        "ax1.axvline(x=50, color='red', linestyle='--', linewidth=2, label='Churn Threshold')\n",
        "ax1.set_xlabel('Churn Risk Score')\n",
        "ax1.set_ylabel('Number of Customers')\n",
        "ax1.set_title('Churn Risk Score Distribution', fontweight='bold')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Feature Importance\n",
        "features = ['Low CSAT', 'High Complaints', 'Unresolved Issues', 'Frequent Caller']\n",
        "weights = [30, 25, 25, 20]\n",
        "ax2.barh(features, weights, color=['red', 'orange', 'coral', 'yellow'])\n",
        "ax2.set_xlabel('Weight in Risk Score')\n",
        "ax2.set_title('Churn Prediction Features', fontweight='bold')\n",
        "ax2.grid(axis='x', alpha=0.3)\n",
        "\n",
        "# 3. CSAT vs Risk Score\n",
        "colors = churn['IS_CHURNED'].map({True: 'red', False: 'green'})\n",
        "ax3.scatter(churn['AVG_CSAT_SCORE'], churn['risk_score'], c=colors, alpha=0.6, s=100)\n",
        "ax3.set_xlabel('Average CSAT Score')\n",
        "ax3.set_ylabel('Churn Risk Score')\n",
        "ax3.set_title('CSAT vs Churn Risk (Red=Churned, Green=Active)', fontweight='bold')\n",
        "ax3.axhline(y=50, color='red', linestyle='--', alpha=0.5)\n",
        "ax3.axvline(x=3, color='orange', linestyle='--', alpha=0.5)\n",
        "ax3.grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Predictions by Segment\n",
        "seg = churn.groupby('CUSTOMER_SEGMENT').agg({\n",
        "    'predicted_churn': 'sum',\n",
        "    'IS_CHURNED': 'sum'\n",
        "})\n",
        "x = range(len(seg))\n",
        "width = 0.35\n",
        "ax4.bar([i-width/2 for i in x], seg['predicted_churn'], width, label='Predicted', color='orange', alpha=0.8)\n",
        "ax4.bar([i+width/2 for i in x], seg['IS_CHURNED'], width, label='Actual', color='red', alpha=0.8)\n",
        "ax4.set_xticks(x)\n",
        "ax4.set_xticklabels(seg.index, rotation=45, ha='right')\n",
        "ax4.set_ylabel('Number of Customers')\n",
        "ax4.set_title('Churn: Predicted vs Actual by Segment', fontweight='bold')\n",
        "ax4.legend()\n",
        "ax4.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.suptitle('Churn Prediction Model', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\\\nUse this model to:\")\n",
        "print(\"- Identify at-risk customers before they churn\")\n",
        "print(\"- Prioritize retention efforts by risk score\")\n",
        "print(\"- Target high-value customers with proactive support\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lab Summary and Completion\n",
        "\n",
        "### What You Accomplished\n",
        "\n",
        "Congratulations! You have completed all 11 exercises and learned to:\n",
        "\n",
        "**Cortex AI Functions:**\n",
        "- AI_TRANSLATE for multilingual support\n",
        "- AI_REDACT for data privacy compliance\n",
        "- AI_SENTIMENT for emotion analysis (VARIANT handling)\n",
        "- Cortex Search for semantic queries across structured and unstructured data\n",
        "\n",
        "**Data Analysis:**\n",
        "- Explored 81,425+ records across multiple tables\n",
        "- Analyzed customer call transcripts and PDF documents\n",
        "- Calculated CSAT scores and sentiment patterns\n",
        "- Identified network performance issues by region\n",
        "\n",
        "**Business Intelligence:**\n",
        "- Identified at-risk customers using customer_360_view\n",
        "- Quantified revenue at risk from potential churn\n",
        "- Built churn prediction model with feature scoring\n",
        "- Created visualizations for executive dashboards\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Structured + Unstructured Integration:** Snowflake seamlessly combines CSV data, audio transcripts, and PDF documents\n",
        "2. **AI-Powered Insights:** Cortex AI functions extract meaning from unstructured text without manual coding\n",
        "3. **Business Impact:** Data directly translates to actionable metrics (revenue at risk, churn probability)\n",
        "4. **Scalability:** Same techniques work for 25 calls or 25,000 calls\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "**1. Use Snowflake Intelligence Agent:**\n",
        "Ask natural language questions like:\n",
        "- \"Which customers in Penang have the highest churn risk?\"\n",
        "- \"What are the top network issues causing customer complaints?\"\n",
        "- \"Show me revenue at risk from business customers\"\n",
        "\n",
        "**2. Build Production Solutions:**\n",
        "- Create scheduled dashboards\n",
        "- Set up alerts for at-risk customers\n",
        "- Automate churn prediction scoring\n",
        "\n",
        "**3. Extend the Analysis:**\n",
        "- Add more data sources (social media, NPS surveys)\n",
        "- Build advanced ML models\n",
        "- Create real-time monitoring dashboards\n",
        "\n",
        "**Thank you for completing the CelcomDigi Snowflake Intelligence Hands-On Lab!**"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
